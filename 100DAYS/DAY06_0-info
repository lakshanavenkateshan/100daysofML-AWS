ðŸ”¹Day 6 â€” Breast Cancer (S3 â†” ML)

ðŸ”¹What I did / goal
Move data â†’ cloud, fetch it for training, do basic feature engineering, train a model locally and save the trained model back to S3.

ðŸ”¹Why I do this

Centralize data (S3) so itâ€™s shareable and reproducible.

Practice boto3 for real-world ML pipelines.

Save models to S3 so they can be deployed later (EC2, Docker, etc.).

ðŸ”¹Files / names

Dataset: refer datasets folder (data)

Local model file: breast_cancer_rf_model.pkl

S3 bucket: your-ml-bucket (replace with yours)

ðŸ”¹High-level steps

Upload dataset to S3 (console or CLI).

aws s3 cp breast_cancer.csv s3://your-ml-bucket/

In Python (download & load): fetch file from S3 with boto3 and load into Pandas.

Code: Refer DAY06.

ðŸ”¹Preprocess & feature engineering (suggestions):

Handle missing values (mean/median).

Map target to 0/1 (e.g., malignant=1, benign=0).

Scale numeric features (StandardScaler) for some algorithms.

Optionally create features (e.g., ratios or domain combos) if you want.

Code: Refer DAY06.

Train a Random Forest classifier (suggest: n_estimators=100, random_state=42) and evaluate: accuracy, precision, recall, ROC-AUC.

Code: Refer DAY06.

ðŸ”¹Save model with joblib.dump("breast_cancer_rf_model.pkl") and upload to S3:

s3.upload_file("breast_cancer_rf_model.pkl","your-ml-bucket","breast_cancer_rf_model.pkl")

Code: Refer DAY06.
