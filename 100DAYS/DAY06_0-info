What Day 6 Was About

Title: Store + Fetch + Train ML model with AWS S3

üîπ The idea was to connect machine learning with the cloud. In real projects:

Data is not sitting on your laptop (too risky, limited storage).

Instead, it is stored in the cloud (AWS S3) so it‚Äôs accessible anytime, anywhere.

ML pipelines fetch this data, process it, train models, and sometimes even store models back in S3 for later deployment.

üîπ That‚Äôs why you learned to:

Upload dataset to S3

Fetch dataset from S3 using boto3 (AWS SDK for Python)

Do feature engineering locally (improve dataset before training)

Save trained ML model (.pkl) back to S3

üîπ Why We Did This

Cloud-first ML workflow ‚Üí Modern ML projects don‚Äôt rely on local storage.

Reproducibility ‚Üí Anyone with AWS creds can access the same dataset.

Scalability ‚Üí Later, instead of training on your laptop, you can train on EC2, SageMaker, or EMR ‚Äî still fetching the same S3 dataset.

Foundation for MLOps ‚Üí This is the first step towards automating ML pipelines (train ‚Üí save ‚Üí deploy).

üîπ How We Did This (Step by Step)

Here‚Äôs what the Day 6 task looked like in practice:

1Ô∏è‚É£ Upload Dataset to S3

Use AWS Console or AWS CLI:

aws s3 mb s3://my-ml-bucket
aws s3 cp titanic.csv s3://my-ml-bucket/

2Ô∏è‚É£ Fetch Dataset in Python with boto3
import boto3
import pandas as pd

s3 = boto3.client('s3')
bucket = "my-ml-bucket"
file_key = "titanic.csv"

# Download file
s3.download_file(bucket, file_key, "titanic.csv")

# Load with pandas
df = pd.read_csv("titanic.csv")
print(df.head())

3Ô∏è‚É£ Feature Engineering

Example:

df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['IsAlone'] = 1  # Assume alone
df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0

4Ô∏è‚É£ Train & Save Model Back to S3
from sklearn.ensemble import RandomForestClassifier
import joblib

X = df[['Pclass','Sex','Age','Fare','FamilySize','IsAlone']].fillna(0)
X['Sex'] = X['Sex'].map({'male':0,'female':1})
y = df['Survived']

model = RandomForestClassifier()
model.fit(X, y)

joblib.dump(model, "titanic_rf_model.pkl")

# Upload model back to S3
s3.upload_file("titanic_rf_model.pkl", bucket, "titanic_rf_model.pkl")

üîπ What You Learned

How AWS S3 integrates with ML workflows

How to use boto3 to fetch & save files

Basics of feature engineering (important for boosting accuracy)

Storing trained models in the cloud (foundation for deployment)
