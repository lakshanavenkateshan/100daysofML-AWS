ðŸ”¹Day 8 â€” Breast Cancer (S3 + EC2 + Jupyter + Cloud Training)

ðŸ”¹What I did / goal
Run Jupyter on an EC2 instance, fetch the breast cancer dataset directly from S3, train a Random Forest on EC2 (bigger compute if needed), and upload the final model back to S3.

ðŸ”¹Why I do this

Shows cloud compute (EC2) used for model training (scalable & repeatable).

Browser-based Jupyter gives full dev experience but runs on cloud.

Keeps dataset & model in S3 â†’ fully cloud-hosted ML workflow.

ðŸ”¹Files / names (suggested)

Same S3 bucket: your-ml-bucket

Notebook on EC2: day08_train_breast_cancer.ipynb

Model saved: breast_cancer_rf_model.pkl (v2 if you updated)

ðŸ”¹High-level steps (no code here â€” see code reference)

Launch EC2, open port for Jupyter (e.g., 8888) or tunnel via SSH.

Install Python, notebook, boto3, pandas, scikit-learn, joblib on EC2.

Start Jupyter Notebook on EC2 and open it in your browser.

In notebook: download dataset from S3 (same as Day 6) and load into Pandas.

Code: Refer DAY08.

Do preprocessing & feature engineering (same approach as Day 6) and split train/test.

Evaluate with accuracy, precision, recall, ROC-AUC.got ~96% earlier, mention it as local/EC2 training result.

Code: Refer DAY08.

Train final Random Forest and save the model locally on EC2 (breast_cancer_rf_model.pkl) then upload to S3.

Code: Refer DAY08.

Download model to local machine or later use for deployment (Day 10/11/12 flow).
