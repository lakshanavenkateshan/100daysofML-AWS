🔹Day 11 — Dockerized ML API on AWS EC2

🔹What You Did / Goal

Took your Titanic ML model and wrapped it in a Flask API.

Made that API containerized using Docker.

Ran the Docker container on AWS EC2.

Tested the API from your local PC using curl → got real predictions.

Basically: you made a production-ready ML application running in the cloud.

🔹Why We Did This

Portability → Docker lets you run your API anywhere (local, EC2, other cloud) without environment issues.

Production readiness → ML models aren’t useful unless they can serve predictions; this is how companies deploy ML in real-world apps.

Integration of ML + DevOps → You’re practicing a key skill: turning ML research into deployable software.

Repeatable workflow → Any ML model can now be wrapped in Docker and deployed the same way.

🔹How We Did This (Step by Step)

▫️Prepare your folder

app.py → Flask API code for Titanic model

titanic_rf_model.pkl → Trained model

Dockerfile → Instructions to build Docker image

Code: Refer DAY11

▫️Build Docker image locally or on EC2

docker build -t titanic-flask-app .


▫️Run Docker container on EC2

docker run -d -p 5000:5000 --restart unless-stopped titanic-flask-app


This exposes port 5000 so the API can be accessed from outside EC2.

▫️Test API from your local PC

curl -X POST http://<EC2-IP>:5000/predict -H "Content-Type: application/json" -d "{\"features\": [3,1,22,0,0,7.25]}"


▫️You got a prediction → means container is running properly.

🔹What You Learned

How to wrap ML models in a Flask API.

How to containerize an ML application with Docker.

How to deploy Docker containers on EC2.

How to test the API from your local machine using JSON input.

This is exactly the workflow that ML engineers + DevOps teams use when taking models from research → production.
